import os
import random
import numpy as np
import shutil
import glob
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.patches as patches
from bs4 import BeautifulSoup
from PIL import Image
import cv2
import time
import torch
import torchvision
from torchvision.models.detection import retinanet_resnet50_fpn
from torchvision.models import ResNet50_Weights
from torch.utils.data import Dataset
from torchvision import transforms
from matplotlib import pyplot as plt
from tqdm import tqdm


print(len(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\annotations')))
print(len(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images')))

random.seed(1234)
idx = random.sample(range(853), 120)

for img in np.array(sorted(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images')))[idx]:
    shutil.move('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images/'+img, 'C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\test_images/'+img)

for annot in np.array(sorted(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\annotations')))[idx]:
    shutil.move('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\annotations/'+annot, 'C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\test_annotations/'+annot)

print(len(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\annotations')))
print(len(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images')))
print(len(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\test_annotations')))
print(len(os.listdir('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\test_images')))

img_list = sorted(glob.glob('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images\\*'))
annot_list = sorted(glob.glob('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\annotations\\*'))


def generate_box(obj):  ## xmin, ymin, xmax, ymax 값 반환

    xmin = float(obj.find('xmin').text)
    ymin = float(obj.find('ymin').text)
    xmax = float(obj.find('xmax').text)
    ymax = float(obj.find('ymax').text)

    return [xmin, ymin, xmax, ymax]


def generate_label(obj):  # 마스크 착용 여부 반환, 0:without mask, 1:with mask, 2:mask_weared_incorrect

    if obj.find('name').text == "with_mask":

        return 1

    elif obj.find('name').text == "mask_weared_incorrect":

        return 2

    return 0


def generate_target(file):  # 위의 두 generate를 각각 호출하여 반환된 값을 저장하고 반환
    with open(file) as f:
        data = f.read()
        soup = BeautifulSoup(data, "html.parser")  # annotations를 불러와 target의 바운딩 박스와 라벨 추출
        objects = soup.find_all("object")

        num_objs = len(objects)

        boxes = []
        labels = []
        for i in objects:
            boxes.append(generate_box(i))
            labels.append(generate_label(i))

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels

        return target


def plot_image_from_output(img, annotation):  # 바운딩 박스 시각화
    # with mask - 빨간색, with mask - 초록색, mask_weared_incorrect - 주황색

    img = img.cpu().permute(1, 2, 0)

    rects = []

    for idx in range(len(annotation["boxes"])):
        xmin, ymin, xmax, ymax = annotation["boxes"][idx]

        if annotation['labels'][idx] == 0:
            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='r',
                                     facecolor='none')

        elif annotation['labels'][idx] == 1:

            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='g',
                                     facecolor='none')

        else:

            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='orange',
                                     facecolor='none')

        rects.append(rect)

    return img, rects


class MaskDataset(Dataset):
    def __init__(self, path, transform=None):
        self.path = path
        self.imgs = list(sorted(os.listdir(self.path)))
        self.transform = transform

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        file_image = self.imgs[idx]
        file_label = self.imgs[idx][:-3] + 'xml'
        img_path = os.path.join(self.path, file_image)

        if 'test' in self.path:
            label_path = os.path.join(
                "C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\test_annotations/", file_label)
        else:
            label_path = os.path.join("C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\annotations/",
                                      file_label)

        img = Image.open(img_path).convert("RGB")
        target = generate_target(label_path)

        to_tensor = torchvision.transforms.ToTensor()

        if self.transform:
            img, transform_target = self.transform(np.array(img), np.array(target['boxes']))
            target['boxes'] = torch.as_tensor(transform_target)

        # tensor로 변경
        img = to_tensor(img)

        return img, target


def collate_fn(batch):
    return tuple(zip(*batch))


dataset = MaskDataset('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images/')
test_dataset = MaskDataset('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\test_images/')

# 훈련용 데이터와 시험용 데이터를 불러올 수 있게 torch.utils.data.DataLoader 함수를 활용해 각각을 정의
data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, collate_fn=collate_fn)
test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=2, collate_fn=collate_fn)

weights_backbone = ResNet50_Weights.IMAGENET1K_V1

retina = retinanet_resnet50_fpn(num_classes=3, weights=None, weights_backbone=weights_backbone)

## torchvision.models 모듈을 통해 RetinaNet모델을 불러옴 Face Mask Detection 데이터셋에 3개의 클래스가 존재하므로 num_classes의 매개변수를 3으로 정의하고 전이학습을 진행하기에 backbone 구조는 사전학습된 가중치를, 그 외 가중치는 초기화시키고 가져옵니다.

device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')

num_epochs = 1
retina.to(device)

# parameters
params = [p for p in retina.parameters() if p.requires_grad]  # gradient calculation이 필요한 params만 추출
optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)

len_dataloader = len(data_loader)

for epoch in range(num_epochs):
    start = time.time()
    retina.train()

    i = 0
    epoch_loss = 0
    for images, targets in data_loader:
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        loss_dict = retina(images, targets)

        losses = sum(loss for loss in loss_dict.values())

        i += 1

        optimizer.zero_grad()
        losses.backward()
        optimizer.step()

        epoch_loss += losses
    print(epoch_loss, f'time: {time.time() - start}')

    torch.save(retina.state_dict(), f'retina_{num_epochs}.pt')

    retina.load_state_dict(torch.load(f'retina_{num_epochs}.pt'))


    def make_prediction(model, img, threshold):
        model.eval()
        preds = model(img)
        for id in range(len(preds)):
            idx_list = []

            for idx, score in enumerate(preds[id]['scores']):
                if score > threshold:  # threshold 넘는 idx 구함.
                    # 신뢰도가 일정 수준(보통 0.5 이상) 이상의 바운딩 박스 선택
                    idx_list.append(idx)

            preds[id]['boxes'] = preds[id]['boxes'][idx_list]
            preds[id]['labels'] = preds[id]['labels'][idx_list]
            preds[id]['scores'] = preds[id]['scores'][idx_list]

        return preds


    labels = []
    preds_adj_all = []
    annot_all = []

    for im, annot in tqdm(test_data_loader, position=0, leave=True):
        # test_data_loader 안에 있는 모든 데이터에 대해 예측 실행
        im = list(img.to(device) for img in im)
        # annot = [{k: v.to(device) for k, v in t.items()} for t in annot]

        for t in annot:
            labels += t['labels']

        with torch.no_grad():  # 예측 값을 preds_adj_all에 저장
            preds_adj = make_prediction(retina, im, 0.5)
            preds_adj = [{k: v.to(torch.device('cpu')) for k, v in t.items()} for t in preds_adj]
            preds_adj_all.append(preds_adj)
            annot_all.append(annot)

nrows = 8
ncols = 2
fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(ncols * 4, nrows * 4))

batch_i = 0
for im, annot in test_data_loader:
    pos = batch_i * 4 + 1
    for sample_i in range(len(im)):

        img, rects = plot_image_from_output(im[sample_i], annot[sample_i])
        axes[(pos) // 2, 1 - ((pos) % 2)].imshow(img)
        for rect in rects:
            axes[(pos) // 2, 1 - ((pos) % 2)].add_patch(rect)

        img, rects = plot_image_from_output(im[sample_i], preds_adj_all[batch_i][sample_i])
        axes[(pos) // 2, 1 - ((pos + 1) % 2)].imshow(img)
        for rect in rects:
            axes[(pos) // 2, 1 - ((pos + 1) % 2)].add_patch(rect)

        pos += 2

    batch_i += 1
    if batch_i == 4:
        break

# xtick, ytick 제거
for idx, ax in enumerate(axes.flat):
    ax.set_xticks([])
    ax.set_yticks([])

colnames = ['True', 'Pred']

for idx, ax in enumerate(axes[0]):
    ax.set_title(colnames[idx])

plt.tight_layout()
plt.show()

# 실제 바운딩 박스와 예측한 바운딩 박스에 대한 시각화

img_list = sorted(glob.glob('C:\\Users\\hitt9\\Desktop\\machine_learning\\Face mask Detection\\images\\*'))
annot_list = sorted(glob.glob('C:\\Users\\hitt9\\Desktop\\machine_learning\\Face mask Detection\\annotations\\*'))


# glob을 이용해 데이터셋을 불러온다.

def generate_box(obj):  ## xmin, ymin, xmax, ymax 값 반환

    xmin = float(obj.find('xmin').text)
    ymin = float(obj.find('ymin').text)
    xmax = float(obj.find('xmax').text)
    ymax = float(obj.find('ymax').text)

    return [xmin, ymin, xmax, ymax]


def generate_label(obj):  # 마스크 착용 여부 반환, 0:without mask, 1:with mask, 2:mask_weared_incorrect
    if obj.find('name').text == "with_mask":
        return 1
    elif obj.find('name').text == "mask_weared_incorrect":
        return 2
    return 0


def generate_target(file):  # 위의 두 generate를 각각 호출하여 반환된 값을 저장하고 반환
    with open(file) as f:
        data = f.read()
        soup = BeautifulSoup(data, "html.parser")  # annotations를 불러와 target의 바운딩 박스와 라벨 추출
        objects = soup.find_all("object")

        num_objs = len(objects)

        boxes = []
        labels = []
        for i in objects:
            boxes.append(generate_box(i))
            labels.append(generate_label(i))

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels

        return target


def plot_image(img_path, annotation):  # 바운딩 박스 시각화
    # without mask - 빨간색, with mask - 초록색, mask_weared_incorrect - 주황색

    img = mpimg.imread(img_path)

    fig, ax = plt.subplots(1)
    ax.imshow(img)

    for idx in range(len(annotation["boxes"])):
        xmin, ymin, xmax, ymax = annotation["boxes"][idx]

        if annotation['labels'][idx] == 0:
            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='r',
                                     facecolor='none')

        elif annotation['labels'][idx] == 1:

            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='g',
                                     facecolor='none')

        else:

            rect = patches.Rectangle((xmin, ymin), (xmax - xmin), (ymax - ymin), linewidth=1, edgecolor='orange',
                                     facecolor='none')

        ax.add_patch(rect)

    plt.show()

    img_list.index('C:\\Users\\USER\\Desktop\\Machine_Learning\\Face mask Detection\\images\\maksssksksss244.png')

    bbox = generate_target(annot_list[117])
    plot_image(img_list[117], bbox)

